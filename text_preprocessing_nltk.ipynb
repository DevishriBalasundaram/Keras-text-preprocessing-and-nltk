{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_tokenizer(doc):\n",
    "    token = Tokenizer()\n",
    "    token.fit_on_texts(doc)\n",
    "    word_index_value = token.word_index\n",
    "    print(colored(\"Word Index:\",\"red\"),colored(word_index_value,\"green\"))\n",
    "    sequences = token.texts_to_sequences(doc)\n",
    "    print(colored(\"The Sequences are:\",\"red\"),colored(sequences,\"green\"))\n",
    "    test_raw_text=[\"But a bit of better butter will make my batter better\",\n",
    "               \"So ‘twas better Betty Botter bought a bit of better butter\",\n",
    "               \"So better butter is good for our health\",]\n",
    "#     test_sequences=token.texts_to_sequences([test_raw_text])\n",
    "    df = pd.DataFrame(data=test_raw_text,columns=['test_raw_data'])\n",
    "    df['test_sequence'] = df.test_raw_data.apply(lambda x: token.texts_to_sequences([x])[0])\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mWord Index:\u001b[0m \u001b[32m{'my': 1, 'batter': 2, 'butter': 3, 'but': 4, 'bitter': 5, 'it': 6, 'will': 7, 'make': 8, 'better': 9, 'betty': 10, 'botter': 11, 'bought': 12, 'some': 13, 'she': 14, 'said': 15, 'the': 16, 'butter’s': 17, 'if': 18, 'i': 19, 'put': 20, 'in': 21, 'a': 22, 'bit': 23, 'of': 24}\u001b[0m\n",
      "\u001b[31mThe Sequences are:\u001b[0m \u001b[32m[[10, 11, 12, 13, 3], [4, 14, 15, 16, 17, 5], [18, 19, 20, 6, 21, 1, 2, 6, 7, 8, 1, 2, 5], [4, 22, 23, 24, 9, 3, 7, 8, 1, 2, 9]]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_raw_data</th>\n",
       "      <th>test_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But a bit of better butter will make my batter...</td>\n",
       "      <td>[4, 22, 23, 24, 9, 3, 7, 8, 1, 2, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So ‘twas better Betty Botter bought a bit of b...</td>\n",
       "      <td>[9, 10, 11, 12, 22, 23, 24, 9, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So better butter is good for our health</td>\n",
       "      <td>[9, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       test_raw_data  \\\n",
       "0  But a bit of better butter will make my batter...   \n",
       "1  So ‘twas better Betty Botter bought a bit of b...   \n",
       "2            So better butter is good for our health   \n",
       "\n",
       "                          test_sequence  \n",
       "0  [4, 22, 23, 24, 9, 3, 7, 8, 1, 2, 9]  \n",
       "1     [9, 10, 11, 12, 22, 23, 24, 9, 3]  \n",
       "2                                [9, 3]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [\"Betty Botter bought some butter\",\"But she said the butter’s bitter\",\n",
    "       \"If I put it in my batter, it will make my batter bitter\",\"But a bit of better butter will make my batter better\"]\n",
    "keras_tokenizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usage of out of vocabulary token (oov_token)\n",
    "def keras_tokenizer_with_oov(doc):\n",
    "    token = Tokenizer(oov_token='UNK')\n",
    "    token.fit_on_texts(doc)\n",
    "    word_index_value = token.word_index\n",
    "    print(colored(\"Word Index:\",\"red\"),colored(word_index_value,\"green\"))\n",
    "    sequences = token.texts_to_sequences(doc)\n",
    "    print(colored(\"The Sequences are:\",\"red\"),colored(sequences,\"green\"))\n",
    "    test_raw_text=[\"But a bit of better butter will make my batter better\",\n",
    "               \"So ‘twas better Betty Botter bought a bit of better butter\",\n",
    "               \"So better butter is good for our health\",]\n",
    "    df = pd.DataFrame(data=test_raw_text,columns=['test_raw_data'])\n",
    "    df['test_sequence'] = df.test_raw_data.apply(lambda x: token.texts_to_sequences([x])[0])\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mWord Index:\u001b[0m \u001b[32m{'UNK': 1, 'my': 2, 'batter': 3, 'butter': 4, 'but': 5, 'bitter': 6, 'it': 7, 'will': 8, 'make': 9, 'better': 10, 'betty': 11, 'botter': 12, 'bought': 13, 'some': 14, 'she': 15, 'said': 16, 'the': 17, 'butter’s': 18, 'if': 19, 'i': 20, 'put': 21, 'in': 22, 'a': 23, 'bit': 24, 'of': 25}\u001b[0m\n",
      "\u001b[31mThe Sequences are:\u001b[0m \u001b[32m[[11, 12, 13, 14, 4], [5, 15, 16, 17, 18, 6], [19, 20, 21, 7, 22, 2, 3, 7, 8, 9, 2, 3, 6], [5, 23, 24, 25, 10, 4, 8, 9, 2, 3, 10]]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_raw_data</th>\n",
       "      <th>test_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But a bit of better butter will make my batter...</td>\n",
       "      <td>[5, 23, 24, 25, 10, 4, 8, 9, 2, 3, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So ‘twas better Betty Botter bought a bit of b...</td>\n",
       "      <td>[1, 1, 10, 11, 12, 13, 23, 24, 25, 10, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So better butter is good for our health</td>\n",
       "      <td>[1, 10, 4, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       test_raw_data  \\\n",
       "0  But a bit of better butter will make my batter...   \n",
       "1  So ‘twas better Betty Botter bought a bit of b...   \n",
       "2            So better butter is good for our health   \n",
       "\n",
       "                               test_sequence  \n",
       "0     [5, 23, 24, 25, 10, 4, 8, 9, 2, 3, 10]  \n",
       "1  [1, 1, 10, 11, 12, 13, 23, 24, 25, 10, 4]  \n",
       "2                  [1, 10, 4, 1, 1, 1, 1, 1]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [\"Betty Botter bought some butter\",\"But she said the butter’s bitter\",\n",
    "       \"If I put it in my batter, it will make my batter bitter\",\"But a bit of better butter will make my batter better\"]\n",
    "keras_tokenizer_with_oov(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_with_padding(doc):\n",
    "    token = Tokenizer(oov_token='UNK')\n",
    "    token.fit_on_texts(doc)\n",
    "    word_index_value = token.word_index\n",
    "    print(colored(\"Word Index:\",\"red\"),colored(word_index_value,\"green\"))\n",
    "    sequences = token.texts_to_sequences(doc)\n",
    "    print(colored(\"The Sequences are:\",\"red\"),colored(sequences,\"green\"))\n",
    "    test_raw_text=[\"As a woodchuck would if a woodchuck could chuck wood\",\n",
    "                   \"he could, and alsoit will chuck as much wood\",\n",
    "                   \"Good wood good chuck its woodchuck\"]\n",
    "    df = pd.DataFrame(data=test_raw_text,columns=['test_raw_data'])\n",
    "    df['test_sequence'] = df.test_raw_data.apply(lambda x: token.texts_to_sequences([x])[0])\n",
    "    max_length = 15\n",
    "    df['pre_padding_seq'] = pad_sequences(df[\"test_sequence\"],\n",
    "                                          maxlen=max_length, \n",
    "                                          padding='pre',\n",
    "                                          truncating='pre').tolist()\n",
    "    df['post_padding_seq'] = pad_sequences(df[\"test_sequence\"],\n",
    "                                          maxlen=max_length, \n",
    "                                          padding='post',\n",
    "                                          truncating='post').tolist()\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mWord Index:\u001b[0m \u001b[32m{'UNK': 1, 'chuck': 2, 'much': 3, 'wood': 4, 'would': 5, 'he': 6, 'as': 7, 'a': 8, 'woodchuck': 9, 'could': 10, 'how': 11, 'if': 12, 'and': 13}\u001b[0m\n",
      "\u001b[31mThe Sequences are:\u001b[0m \u001b[32m[[11, 3, 4, 5, 8, 9, 2, 12, 8, 9, 10, 2, 4, 6, 5, 2, 6, 5, 7, 3, 7, 6, 10, 13, 2, 7, 3, 4]]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_raw_data</th>\n",
       "      <th>test_sequence</th>\n",
       "      <th>pre_padding_seq</th>\n",
       "      <th>post_padding_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a woodchuck would if a woodchuck could chuc...</td>\n",
       "      <td>[7, 8, 9, 5, 12, 8, 9, 10, 2, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 7, 8, 9, 5, 12, 8, 9, 10, 2, 4]</td>\n",
       "      <td>[7, 8, 9, 5, 12, 8, 9, 10, 2, 4, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he could, and alsoit will chuck as much wood</td>\n",
       "      <td>[6, 10, 13, 1, 1, 2, 7, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 6, 10, 13, 1, 1, 2, 7, 3, 4]</td>\n",
       "      <td>[6, 10, 13, 1, 1, 2, 7, 3, 4, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good wood good chuck its woodchuck</td>\n",
       "      <td>[1, 4, 1, 2, 1, 9]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 2, 1, 9]</td>\n",
       "      <td>[1, 4, 1, 2, 1, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       test_raw_data  \\\n",
       "0  As a woodchuck would if a woodchuck could chuc...   \n",
       "1       he could, and alsoit will chuck as much wood   \n",
       "2                 Good wood good chuck its woodchuck   \n",
       "\n",
       "                      test_sequence  \\\n",
       "0  [7, 8, 9, 5, 12, 8, 9, 10, 2, 4]   \n",
       "1     [6, 10, 13, 1, 1, 2, 7, 3, 4]   \n",
       "2                [1, 4, 1, 2, 1, 9]   \n",
       "\n",
       "                                   pre_padding_seq  \\\n",
       "0  [0, 0, 0, 0, 0, 7, 8, 9, 5, 12, 8, 9, 10, 2, 4]   \n",
       "1  [0, 0, 0, 0, 0, 0, 6, 10, 13, 1, 1, 2, 7, 3, 4]   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 2, 1, 9]   \n",
       "\n",
       "                                  post_padding_seq  \n",
       "0  [7, 8, 9, 5, 12, 8, 9, 10, 2, 4, 0, 0, 0, 0, 0]  \n",
       "1  [6, 10, 13, 1, 1, 2, 7, 3, 4, 0, 0, 0, 0, 0, 0]  \n",
       "2    [1, 4, 1, 2, 1, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [\"How much wood would a woodchuck chuck if a woodchuck could chuck wood? He would chuck, he would, as much as he could, and chuck as much wood\"]\n",
    "tokenizer_with_padding(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is basically removing the suffix from a word and reduce it to its root word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Porter Stemmer\n",
    "Most common and gentle stemmer.Its fast but not very precise and always correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener provis maximum multipli owe care on go gone go wa thi univers univers univers alumnu alumni alumna\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'like', 'like', 'like', 'like', 'democrat', 'plot', 'stripe']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "sentence = \"generously Provision Maximum multiply owed caring on go gone going was this universal university universe alumnus alumni alumnae\"\n",
    "# wordList = nltk.word_tokenize(sentence)\n",
    "# stemWords = [porter.stem(word) for word in wordList]\n",
    "stemWords = [porter.stem(word) for word in sentence.split()]\n",
    "print(' '.join(stemWords))\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'likes','liked','likely','liking','democratization',\n",
    "           'plotted','stripes']\n",
    "singles = [porter.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Snowball Stemmer\n",
    "More precise and meaningfull over large data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSnowballStemmer supported languages:\u001b[0m arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n",
      "generous provis maximum multipli owe care on go gone go was this univers univers univers alumnus alumni alumna\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'like', 'like', 'like', 'like', 'democrat', 'plot', 'stripe']\n",
      "\u001b[31mGerman:\u001b[0m gut morg\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer(\"english\")\n",
    "print(colored(\"SnowballStemmer supported languages:\",\"red\"),\" \".join(SnowballStemmer.languages))\n",
    "sentence = \"Generously Provision Maximum multiply owed caring on go gone going was this universal university universe alumnus alumni alumnae\"\n",
    "stemWords = [snow.stem(word) for word in sentence.split()]\n",
    "print(' '.join(stemWords))\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'likes','liked','likely','liking','democratization',\n",
    "           'plotted','stripes']\n",
    "singles = [snow.stem(plural) for plural in plurals]\n",
    "print(singles)\n",
    "\n",
    "# German language\n",
    "snow_german = SnowballStemmer(\"german\")\n",
    "wordlists = \"Guten Morgen\"\n",
    "root = [snow_german.stem(word) for word in wordlists.split()]\n",
    "print(colored(\"German:\",\"red\"),' '.join(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lancaster Stemmmer\n",
    "very aggressive (really confusing when dealing with small words) and it will hugely trim down so that in most of the cases it gives wrong meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen provid maxim multiply ow car on go gon going was thi univers univers univers alumn alumn alumna\n",
      "['caress', 'fli', 'die', 'mul', 'deny', 'died', 'agree', 'own', 'humbl', 'siz', 'meet', 'stat', 'siez', 'item', 'sens', 'tradit', 'ref', 'colon', 'lik', 'lik', 'lik', 'lik', 'democr', 'plot', 'stripes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "sentence = \"generously Provision Maximum multiply owed caring on go gone going was this universal university universe alumnus alumni alumnae\"\n",
    "stemWords = [lancaster.stem(word) for word in sentence.split()]\n",
    "print(' '.join(stemWords))\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'likes','liked','likely','liking','democratization',\n",
    "           'plotted','stripes']\n",
    "singles = [lancaster.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mLemma NOUN Words:\u001b[0m ['am', 'are', 'is', 'foot', 'car', 'car', \"car's\", \"cars'\", 'different', 'color', 'corpus', 'better', 'play', 'playing', 'stripe', 'pony', 'care', 'denied', 'sensational', 'traditional', 'reference', 'colonizer', 'like', 'liked', 'likely', 'liking', 'democratization', 'plotted', 'hang', 'one', 'fish']\n",
      "\u001b[31mLemma VERB Words:\u001b[0m ['be', 'be', 'be', 'feet', 'car', 'cars', \"car's\", \"cars'\", 'different', 'color', 'corpora', 'better', 'play', 'play', 'strip', 'ponies', 'care', 'deny', 'sensational', 'traditional', 'reference', 'colonizer', 'like', 'like', 'likely', 'like', 'democratization', 'plot', 'hang', 'ones', 'fish']\n",
      "\u001b[31mLemma ADJECTIVE Words:\u001b[0m ['am', 'are', 'is', 'feet', 'car', 'cars', \"car's\", \"cars'\", 'different', 'colors', 'corpora', 'good', 'plays', 'playing', 'stripes', 'ponies', 'cares', 'denied', 'sensational', 'traditional', 'reference', 'colonizer', 'likes', 'liked', 'likely', 'liking', 'democratization', 'plotted', 'hangs', 'ones', 'fishes']\n",
      "\u001b[31mLemma ADVERB Words:\u001b[0m ['am', 'are', 'is', 'feet', 'car', 'cars', \"car's\", \"cars'\", 'different', 'colors', 'corpora', 'well', 'plays', 'playing', 'stripes', 'ponies', 'cares', 'denied', 'sensational', 'traditional', 'reference', 'colonizer', 'likes', 'liked', 'likely', 'liking', 'democratization', 'plotted', 'hangs', 'ones', 'fishes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "docs = [\"am\",\"are\",\"is\",\"feet\",\"car\",\"cars\",\"car's\",\"cars'\",\"different\",\"colors\",\"corpora\",\"better\",\"plays\",\"playing\",\"stripes\",\n",
    "       \"ponies\",\"cares\",\"denied\",'sensational', 'traditional', 'reference', 'colonizer','likes','liked','likely','liking',\n",
    "        'democratization','plotted','hangs','ones','fishes']\n",
    "\n",
    "lemma_noun_words = [lemma.lemmatize(doc,pos=\"n\") for doc in docs]\n",
    "print(colored(\"Lemma NOUN Words:\",\"red\"),lemma_noun_words)\n",
    "\n",
    "lemma_verb_words = [lemma.lemmatize(doc,pos=\"v\") for doc in docs]\n",
    "print(colored(\"Lemma VERB Words:\",\"red\"),lemma_verb_words)\n",
    "\n",
    "lemma_adj_words = [lemma.lemmatize(doc,pos=\"a\") for doc in docs]\n",
    "print(colored(\"Lemma ADJECTIVE Words:\",\"red\"),lemma_adj_words)\n",
    "\n",
    "lemma_adv_words = [lemma.lemmatize(doc,pos=\"r\") for doc in docs]\n",
    "print(colored(\"Lemma ADVERB Words:\",\"red\"),lemma_adv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
